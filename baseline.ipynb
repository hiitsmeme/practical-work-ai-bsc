{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds = random.sample(range(1, 10000), 5) # code used to generate seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [8479, 227, 5413, 8179, 7528]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading computed features\n",
    "data = np.load('./data/data_scaled.npz')\n",
    "X_train_scaled = data['X_train'][:40000]\n",
    "X_test_scaled = data['X_test']\n",
    "y_train = data['y_train'][:40000]\n",
    "y_test = data['y_test']\n",
    "y_finetuning1 = data['y_finetuning1']\n",
    "y_finetuning2 = data['y_finetuning2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to collect results per task\n",
    "all_results = {\n",
    "        \"Delta-AUC-PR\": [],\n",
    "        \"ROC-AUC\": []\n",
    "    }  \n",
    "# list to collect results avg over task\n",
    "avg_results = {\n",
    "        \"Delta-AUC-PR\": [],\n",
    "        \"ROC-AUC\": []\n",
    "    }\n",
    "\n",
    "# train RF five times and avg results\n",
    "for run in range(5):\n",
    "    results = []\n",
    "    for i in range(9):\n",
    "        # remove samples with NaN train\n",
    "        mask_valid_train = ~np.isnan(y_train[:, i])\n",
    "        X_train_scaled_valid = X_train_scaled[mask_valid_train]\n",
    "        y_train_i = y_train[:, i][mask_valid_train]\n",
    "        \n",
    "        # remove samples with Nan test\n",
    "        mask_valid_test = ~np.isnan(y_test[:, i])\n",
    "        X_test_scaled_valid = X_test_scaled[mask_valid_test]\n",
    "        y_test_i = y_test[:, i][mask_valid_test]\n",
    "        \n",
    "        # get stats for how many samples used\n",
    "        deleted_train_count = len(y_train) - len(y_train_i)\n",
    "        deleted_train_pct = round(deleted_train_count / len(y_train) * 100, 2)\n",
    "        deleted_test_count = len(y_test) - len(y_test_i)\n",
    "        deleted_test_pct = round(deleted_test_count / len(y_test) * 100, 2)\n",
    "        \n",
    "        # fraction of positive samples in train set (-> how rare is label 1 vs label 0)\n",
    "        mask_pos_train = (y_train_i == 1)\n",
    "        percent_pos_train = len(y_train_i[mask_pos_train]) / len(y_train_i)\n",
    "        \n",
    "        # rf \n",
    "        rf = RandomForestClassifier(random_state=seeds[run])\n",
    "        rf.fit(X_train_scaled_valid, y_train_i)\n",
    "        \n",
    "        # predictions & metrics\n",
    "        y_pred_proba = rf.predict_proba(X_test_scaled_valid)[:, 1]\n",
    "        auc = roc_auc_score(y_test_i, y_pred_proba)\n",
    "        delta_auc_pr = average_precision_score(y_test_i, y_pred_proba) - percent_pos_train\n",
    "        \n",
    "        # factor: better or worse than random: <1: worse\n",
    "        factor = delta_auc_pr / percent_pos_train\n",
    "        factor = round(factor, 2)\n",
    "        \n",
    "        # results dict\n",
    "        results.append({\n",
    "            \"Task\": i + 1,\n",
    "            \"used train (#)\": deleted_train_count,\n",
    "            \"used train (%)\": deleted_train_pct,\n",
    "            \"used test (#)\": deleted_test_count,\n",
    "            \"used test (%)\": deleted_test_pct,\n",
    "            \"Pos. Labels/Train\": round(percent_pos_train, 6),\n",
    "            \"ROC-AUC\": round(auc, 4),\n",
    "            \"Delta-AUC-PR\": round(delta_auc_pr, 6),\n",
    "            \"Factor Better/Worse\": factor\n",
    "        })\n",
    "\n",
    "        df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # collect results for all tasks\n",
    "    all_results[\"Delta-AUC-PR\"].append(np.array(df_results[\"Delta-AUC-PR\"]))\n",
    "    all_results[\"ROC-AUC\"].append(np.array(df_results[\"ROC-AUC\"]))\n",
    "\n",
    "    # collect results avg over tasks\n",
    "    mean_auc_pr = np.mean(np.array(df_results[\"Delta-AUC-PR\"]))\n",
    "    mean_roc_auc = np.mean(np.array(df_results[\"ROC-AUC\"]))\n",
    "    avg_results[\"Delta-AUC-PR\"].append(mean_auc_pr)\n",
    "    avg_results[\"ROC-AUC\"].append(mean_roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute avg and sd over five runs\n",
    "final_auc_pr = np.mean(np.array(avg_results[\"Delta-AUC-PR\"]))\n",
    "final_roc_auc = np.mean(np.array(avg_results[\"ROC-AUC\"]))\n",
    "final_sd_auc_pr = np.std(np.array(avg_results[\"Delta-AUC-PR\"]))\n",
    "final_sd_roc_auc = np.std(np.array(avg_results[\"ROC-AUC\"]))\n",
    "\n",
    "# save in json convertible format\n",
    "final_results = [{\n",
    "    \"Delta-AUC-PR\": final_auc_pr,\n",
    "    \"ROC-AUC\": final_roc_auc,\n",
    "    \"Sd-Delta-AUC-PR\": final_sd_auc_pr,\n",
    "    \"Sd-ROC-AUC\": final_sd_roc_auc\n",
    "}]\n",
    "\n",
    "# save to json file\n",
    "with open(\"./metrics/metrics_baseline.json\", \"w\") as f:\n",
    "    json.dump(final_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {\n",
    "    key: [arr.tolist() if isinstance(arr, np.ndarray) else arr for arr in value]\n",
    "    for key, value in all_results.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and sd for five runs but don't average over tasks\n",
    "avg_auc_pr_per_task = np.mean(np.array(all_results[\"Delta-AUC-PR\"]), axis=0)\n",
    "sd_auc_pr_per_task = np.std(np.array(all_results[\"Delta-AUC-PR\"]), axis=0)\n",
    "avg_roc_auc_per_task = np.mean(np.array(all_results[\"ROC-AUC\"]), axis=0)\n",
    "sd_roc_auc_per_task = np.std(np.array(all_results[\"ROC-AUC\"]), axis=0)\n",
    "\n",
    "# save in json convertible format\n",
    "final_results_per_task = [{\n",
    "    \"Delta-AUC-PR per task\": list(avg_auc_pr_per_task),\n",
    "    \"ROC-AUC per task\": list(avg_roc_auc_per_task),\n",
    "    \"Sd-Delta-AUC-PR per task\": list(sd_auc_pr_per_task),\n",
    "    \"Sd-ROC-AUC per task\": list(sd_roc_auc_per_task)\n",
    "}]\n",
    "final_results_per_task\n",
    "\n",
    "# write to json file\n",
    "with open(\"./metrics/metrics_baseline_per_task.json\", \"w\") as f:\n",
    "    json.dump(final_results_per_task, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and sd for five runs but do average over tasks\n",
    "avg_auc_pr_per_run = np.mean(np.array(all_results[\"Delta-AUC-PR\"]), axis=1)\n",
    "sd_auc_pr_per_run = np.std(np.array(all_results[\"Delta-AUC-PR\"]), axis=1)\n",
    "\n",
    "avg_roc_auc_per_run = np.mean(np.array(all_results[\"ROC-AUC\"]), axis=1)\n",
    "sd_roc_auc_per_run = np.std(np.array(all_results[\"ROC-AUC\"]), axis=1)\n",
    "\n",
    "# save in json convertible format\n",
    "final_results_per_task = [{\n",
    "    \"Delta-AUC-PR per run\": list(avg_auc_pr_per_run),\n",
    "    \"ROC-AUC per run\": list(avg_roc_auc_per_run),\n",
    "    \"Sd-Delta-AUC-PR per run\": list(sd_auc_pr_per_run),\n",
    "    \"Sd-ROC-AUC per run\": list(sd_roc_auc_per_run)\n",
    "}]\n",
    "final_results_per_task\n",
    "\n",
    "# write to json file\n",
    "with open(\"./metrics/metrics_baseline_per_run.json\", \"w\") as f:\n",
    "    json.dump(final_results_per_task, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
